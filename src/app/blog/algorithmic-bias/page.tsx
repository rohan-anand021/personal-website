"use client";
import { useState } from "react";

const post = {
  title: "The Implications of Bias on Hiring at Amazon",
  pdfPath: "/AlgorithmicBias.pdf",
  content: `
    As one of the fastest growing and ubiquitous technology companies worldwide, Amazon seeks to grow year after year by adding an increasing number of jobs. During their Career Day event in 2021, Amazon announced they received over 1 million job applications. The growing volume of applicants begets hiring additional members to their HR team, driving company expenses up. This, combined with the boom in machine learning and AI, prompted Amazon to seek out alternatives to efficiently streamline applications without the additional costs. As a result, in 2014, Amazon began to develop an automated system to review applicants' resumes. This system used Al to rate job applications from 1-5, with the goal of creating an ordered list of the applicants in order to hire the best predetermined percentage. The end goal of the system, according to an Amazon employee, was that "They literally wanted it to be an engine where I'm going to give you 100 resumes, it will spit out the top five, and we'll hire those." However, by 2015, Amazon found that for software-engineering jobs, as well as other technical roles, the algorithm was not hiring in a gender-neutral manner. For example, since it was trained on data of resumes that were considered successful hires, which in Amazon's case was majority men due to the inherent gender bias in industry technical roles, it learned to penalize any mention of woman in the application, whether it be captain of the women's chess club, or attending an all-women's college. The Al engineers tried their best to correct the biases, but, due to the nature of ambiguity surrounding the output of Al algorithms, were ultimately unable to guarantee a gender-neutral result. At the start of 2017, the team disbanded the project, as the perceived benefit did not outweigh the potential lawsuits for discrimination, but continued to use similar algorithms to provide recommendations to the HR team, who would still be responsible for making the final decision. However, Al development is still in its developmental stages with AGI, AI that possesses reasoning capabilities similar to humans, still in the far future, meaning that there is no reason Amazon will not reconsider introducing Al as the primary hiring mechanism. Amazon, a key innovator in the technology space as shown by its many products such as Alexa, one of the first Al-integrated NLP systems available to consumers, shows no incompetence in the development of complex Al programs. Similarly, its hiring algorithm can be reasonably believed to have these complexities, but its failure lies in its training data, which can be arguably more important than the intricacies of the algorithm itself. Fundamentally, Amazon sought to answer this simple question: How can we identify and select resumes of applicants who will bring the most business value to Amazon? The answer, simple enough, is to look at past applicants who have succeeded at the highest level. That is, current employees who have flown through the ranks, have been leaders and achievers in key projects, and who command the highest pay. However, this metric poses a significant bias: most software engineers, through no fault of Amazon, are men. According to the U.S. Bureau of Labor statistics, in 2020, 80.6% of software developers were men, compared to only 19.4% of women. So, more men are hired at Amazon simply, not because women are unable to perform as engineers, but rather the top applicants will be majority men. As a result, when the algorithm seeks out patterns in hiring, it will prefer male candidates because those candidates are the ones who were selected previously, are promoted through the ranks and are therefore paid the highest. This natural inequality perpetuates up the ranks, leaving no ability for the algorithm to notice patterns of a greater percentage female candidates perhaps becoming software product managers, directors, etc. This bias cannot simply be addressed with the removal of names either. The complexity of the algorithm causes it to find even the smallest instances of gendered patterns, perpetuating bias. For example, other variables such as hobbies, extracurricular activities, etc. are more closely associated with females, effectively serving as a proxy. Even at a fundamental level, the algorithm cannot be modified to account for a natural gender bias. Gender bias in STEM arises from a number of factors from the development of children, to the lack of representation. Predominantly, males are encouraged to pursue activities that promote spatial skills and complex-problem solving, while females are encouraged to pursue activities that are more social. This early nurturing significantly shapes the child's academic proclivities, and, as a result, men are more likely to learn towards studying STEM, while females may lean towards the social sciences. In the future, this creates a lack of representation of successful women in male- dominated fields, deterring young women from considering these careers as viable options. There currently exists no mechanism to implement this reasoning into Al algorithms, as they are effectively only pattern recognition machines, turning data into inequalities to produce a desired output. As a result, the bias in hiring is an immutable characteristic, and only with the continual research in introducing logical reasoning, and its subsequent improvement on AI algorithms, can tech companies begin to dismiss recruiters in their hiring process. Proponents of the continued development of the hiring algorithm might claim that these biases are insignificant, and do not change the outcome. However, in my view, this is unethical. As a believer in deontological ethics, the two maxims that guide my decisions are treat individuals as ends in and of themselves, and perform or condone actions that can be done to all people without exception. Essentially, treat others with the same respect that you would like yourself to be treated with. As a result, implementing a hiring algorithm as the sole agent responsible for selecting applications fails both maxims. On the former, reducing the intricacies of human experience and the knowledge, both social and academic, that life provides into (seemingly) quantifiable stats on a piece of paper violates the inherent dignity of individuals by failing to recognizing them as humans. While the algorithm is able to assess technical skills with proficiency, many of the soft skills, such as leadership, communication, and relatability on a human level, which are key indicators of success on group projects as well as qualities that managers heavily consider when deciding on promotion, are lost, objectifying both men and women in the hiring process. Similarly, the inherent bias in the algorithm that fails to consider women as viable candidates perpetuates and reinforces existing gender inequality, particularly objectifying women as they are considered inferior to men. Women, like men, are also individuals worthy of respect and equal consideration in the hiring process. On the latter, unfairly considering men and women equally promotes discrimination, a characteristic of the process that can also disadvantage others. As a minority myself, there was a time in this country that individuals such as myself would be considered inferior, regardless of the equality of even superiority of my skills compared to someone of the dominant race, and would therefore would be passed over due to my immutable characteristics. I would not want that situation that happen to me, therefore, ethically, I cannot excuse that similar outcome for someone of a different marginalized identity. Fundamentally, implementing a biased hiring algorithm violates the inherent respect that all individuals deserve, and advocating for a system that benefits males such as myself, while discriminating against females is incompatible with my deontological ethics. As development in Al continues to move at a rapid pace, Amazon will no doubt reconsider implementing improved versions of a similar hiring algorithm. These changes can be promising, but in the near future, there can be little improvement in terms of fairness without the capability of Al to reason, as the algorithm is able to discriminate regardless of whether the names, colleges, and other gendered descriptors have been obscured, due to its ability to learn the nuanced associations between genders and their preferred activities and experiences. However, Amazon can try to mitigate bias in its current model by implementing key changes in the data pipeline. First, Amazon can modify the training the data to be more inclusive. That is, changing the original metric from which candidates within Amazon perform the best to an alternate metric, such that the resulting candidates that answer that metric are more diverse in terms of gender. It is unclear what the exact split in proportion based on gender that the resulting candidates in the data need to have, but ultimately using alternate data that has more balanced gender percentages will greatly help minimize bias, as the algorithm will stop (or, at the minimum, extremely lightly) penalize female candidates. Secondly, the engineers can try obscuring the features even further. That is, modifying the resumes beyond removing names, colleges, etc., by creating more neutral descriptions of female dominant experiences using an LLM. Doing so will further mask associations between a resume and the gender of its applicant, consequently reducing the penalization of the candidate, similar to the first method. Thirdly, Amazon can reduce the number of recruiters involved in the hiring process, but should still defer to them to make the final decision. More importantly, training them to recognize signs of Al bias in the resumes the model has selected is key to prevent discrimination. Lastly, a more nuanced machine learning approach called adversarial debiasing can be implemented. This adds extra parameters that are meant to reduce gender bias, while maintaining accuracy. The model attempts to highly correlate candidate resumes with internal resumes with high scores, while minimizing its accuracy in predicting gender. However, it is (extremely) reasonably plausible that Amazon, being a pioneer in ML, has attempted this, and, given the state of the program, has had no success. Implementing these changes in the modeling pipeline will reduce bias at the expense of accuracy, a trade-off that Amazon must ultimately decide to make before logical reasoning can be implemented into AI. Amazon's mission to create a hiring algorithm was ultimately a failure due to the inherent bias in the training data due to social structures. While it can serve to ease the workload of recruiters if modified through alternate training data, obscuring features, and continued human involvement in the hiring- ultimately it is unable to provide a fair assessment in all cases, eliminating the possibility of it replacing recruitment entirely. As a result, while Al continues to develop, Amazon should refrain from solely relying on hiring algorithms, as it violates deontological ethics.
  `,
};

const HtmlContent = ({ content }: { content: string }) => (
  <div className="prose lg:prose-lg max-w-none text-gray-700 leading-relaxed space-y-4">
    {content.split("\n").map((paragraph, index) => (
      <p key={index}>{paragraph}</p>
    ))}
  </div>
);

export default function BlogPost() {
  const [showPdf, setShowPdf] = useState(false);
  return (
    <article className="my-12">
      <header className="border-b pb-4 mb-8">
        <h1 className="text-4xl font-bold">{post.title}</h1>
        <div className="flex items-center gap-4 mt-4">
          <button
            onClick={() => setShowPdf(!showPdf)}
            className="inline-block bg-gray-200 text-gray-800 font-semibold px-5 py-2 rounded-lg hover:bg-gray-300 transition-colors"
          >
            {showPdf ? "View HTML" : "View Original PDF"}
          </button>
          <a
            href={post.pdfPath}
            download
            className="inline-block bg-blue-600 text-white font-semibold px-5 py-2 rounded-lg hover:bg-blue-700 transition-colors"
          >
            Download PDF
          </a>
        </div>
      </header>

      <div className="bg-white p-8 sm:p-12 border border-gray-200 rounded-lg shadow-sm">
        {showPdf ? (
          <iframe
            src={post.pdfPath}
            className="w-full h-[1000px]"
            title={post.title}
          ></iframe>
        ) : (
          <HtmlContent content={post.content} />
        )}
      </div>
    </article>
  );
}
